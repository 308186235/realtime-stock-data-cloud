#!/usr/bin/env python3
"""
è¶…é«˜æ€§èƒ½5000+è‚¡ç¥¨æ¨é€æ¥æ”¶å™¨
ä¸“é—¨è§£å†³100Mæ•°æ®å †ç§¯å’ŒæœåŠ¡ç«¯æ–­å¼€è¿æ¥é—®é¢˜
"""

import asyncio
import redis
import time
import threading
import queue
import psutil
import logging
from typing import Dict, Any
from collections import deque
import gc

# é…ç½®æœ€å°åŒ–æ—¥å¿—ï¼Œé¿å…å½±å“æ€§èƒ½
logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UltraHighPerformanceReceiver:
    def __init__(self):
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.max_memory_mb = 80  # æœ€å¤§å†…å­˜ä½¿ç”¨80MBï¼Œè¿œä½äº100MBé™åˆ¶
        self.max_queue_size = 50000  # æœ€å¤§é˜Ÿåˆ—å¤§å°
        self.redis_batch_size = 2000  # Redisæ‰¹é‡å¤§å°
        self.memory_check_interval = 1  # æ¯ç§’æ£€æŸ¥å†…å­˜
        
        # Redisè¿æ¥æ±  - é«˜æ€§èƒ½é…ç½®
        self.redis_client = redis.Redis(
            host='localhost',
            port=6379,
            db=3,  # ä¸“ç”¨æ•°æ®åº“
            decode_responses=False,  # äºŒè¿›åˆ¶æ¨¡å¼
            max_connections=100,  # å¢åŠ è¿æ¥æ•°
            socket_keepalive=True,
            socket_keepalive_options={},
            health_check_interval=30,
            retry_on_timeout=True,
            socket_connect_timeout=2,
            socket_timeout=2
        )
        
        # è¶…é«˜é€Ÿé˜Ÿåˆ—
        self.raw_data_queue = deque(maxlen=self.max_queue_size)
        self.processing_queue = queue.Queue(maxsize=10000)
        
        # è¿è¡ŒçŠ¶æ€
        self.running = False
        self.threads = []
        
        # æ€§èƒ½ç»Ÿè®¡ - æœ€å°åŒ–
        self.stats = {
            'received': 0,
            'stored': 0,
            'memory_mb': 0,
            'queue_size': 0,
            'last_check': 0
        }
        
        # å†…å­˜ç›‘æ§
        self.process = psutil.Process()
        
    def start(self):
        """å¯åŠ¨è¶…é«˜æ€§èƒ½æ¥æ”¶å™¨"""
        logger.warning("ğŸš€ å¯åŠ¨5000+è‚¡ç¥¨è¶…é«˜æ€§èƒ½æ¥æ”¶å™¨...")
        
        self.running = True
        
        # å¯åŠ¨çº¿ç¨‹
        self.threads = [
            threading.Thread(target=self._receive_loop, daemon=True),
            threading.Thread(target=self._redis_store_loop, daemon=True),
            threading.Thread(target=self._memory_monitor_loop, daemon=True)
        ]
        
        for thread in self.threads:
            thread.start()
        
        logger.warning("âœ… è¶…é«˜æ€§èƒ½æ¥æ”¶å™¨å¯åŠ¨å®Œæˆ")
    
    def stop(self):
        """åœæ­¢æ¥æ”¶å™¨"""
        self.running = False
        for thread in self.threads:
            if thread.is_alive():
                thread.join(timeout=5)
    
    def receive_data(self, raw_data: bytes):
        """æ¥æ”¶åŸå§‹æ•°æ® - è¶…é«˜é€Ÿå¤„ç†"""
        try:
            # ç«‹å³æ”¾å…¥é˜Ÿåˆ—ï¼Œç»å¯¹ä¸åšä»»ä½•å¤„ç†ï¼
            self.raw_data_queue.append({
                'data': raw_data,
                'timestamp': time.time(),
                'size': len(raw_data)
            })
            
            # æœ€å°åŒ–ç»Ÿè®¡æ›´æ–°
            self.stats['received'] += 1
            
        except Exception:
            # é™é»˜å¤„ç†é”™è¯¯ï¼Œä¸å½±å“æ€§èƒ½
            pass
    
    def _receive_loop(self):
        """æ¥æ”¶å¾ªç¯ - ä¸“é—¨å¤„ç†é˜Ÿåˆ—åˆ°Redis"""
        batch_data = []
        last_store_time = time.time()
        
        while self.running:
            try:
                # å¿«é€Ÿæ”¶é›†æ•°æ®
                while len(batch_data) < self.redis_batch_size and self.raw_data_queue:
                    try:
                        data_item = self.raw_data_queue.popleft()
                        batch_data.append(data_item)
                    except IndexError:
                        break
                
                # æ‰¹é‡å­˜å‚¨åˆ°Redis
                if batch_data and (len(batch_data) >= self.redis_batch_size or 
                                 time.time() - last_store_time > 0.5):
                    self._store_batch_to_redis(batch_data)
                    batch_data = []
                    last_store_time = time.time()
                
                # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(0.001)  # 1æ¯«ç§’
                
            except Exception:
                # é™é»˜å¤„ç†ï¼Œä¸å½±å“æ€§èƒ½
                pass
    
    def _store_batch_to_redis(self, batch_data: list):
        """æ‰¹é‡å­˜å‚¨åˆ°Redis - è¶…é«˜æ€§èƒ½"""
        try:
            # ä½¿ç”¨pipelineæœ€å¤§åŒ–æ€§èƒ½
            pipe = self.redis_client.pipeline()
            
            for i, data_item in enumerate(batch_data):
                # ç›´æ¥å­˜å‚¨åŸå§‹æ•°æ®ï¼Œä¸åšä»»ä½•è§£æ
                pipe.lpush('stock:raw:queue', data_item['data'])
                
                # æ¯1000æ¡æ‰§è¡Œä¸€æ¬¡ï¼Œé¿å…pipelineè¿‡å¤§
                if (i + 1) % 1000 == 0:
                    pipe.execute()
                    pipe = self.redis_client.pipeline()
            
            # æ‰§è¡Œå‰©ä½™å‘½ä»¤
            if len(batch_data) % 1000 != 0:
                pipe.execute()
            
            # é™åˆ¶é˜Ÿåˆ—é•¿åº¦ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
            self.redis_client.ltrim('stock:raw:queue', 0, 100000)
            
            self.stats['stored'] += len(batch_data)
            
        except Exception:
            # é™é»˜å¤„ç†Redisé”™è¯¯
            pass
    
    def _redis_store_loop(self):
        """Rediså­˜å‚¨å¾ªç¯"""
        while self.running:
            try:
                # æ£€æŸ¥Redisè¿æ¥
                self.redis_client.ping()
                
                # æ¸…ç†è¿‡æœŸæ•°æ®
                if self.stats['stored'] % 10000 == 0:
                    self._cleanup_redis_data()
                
                time.sleep(5)  # 5ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception:
                # Redisè¿æ¥é—®é¢˜ï¼Œå°è¯•é‡è¿
                try:
                    self.redis_client = redis.Redis(
                        host='localhost',
                        port=6379,
                        db=3,
                        decode_responses=False,
                        max_connections=100
                    )
                except Exception:
                    pass
                
                time.sleep(1)
    
    def _memory_monitor_loop(self):
        """å†…å­˜ç›‘æ§å¾ªç¯ - é˜²æ­¢è¶…è¿‡100M"""
        while self.running:
            try:
                # è·å–å†…å­˜ä½¿ç”¨
                memory_info = self.process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                
                self.stats['memory_mb'] = memory_mb
                self.stats['queue_size'] = len(self.raw_data_queue)
                self.stats['last_check'] = time.time()
                
                # å†…å­˜è¶…è¿‡70MBæ—¶å¼€å§‹æ¸…ç†
                if memory_mb > 70:
                    self._emergency_cleanup()
                
                # å†…å­˜è¶…è¿‡80MBæ—¶å¼ºåˆ¶æ¸…ç†
                if memory_mb > 80:
                    self._force_cleanup()
                
                time.sleep(self.memory_check_interval)
                
            except Exception:
                pass
    
    def _emergency_cleanup(self):
        """ç´§æ€¥æ¸…ç† - é˜²æ­¢è¾¾åˆ°100Mé™åˆ¶"""
        try:
            # æ¸…ç†é˜Ÿåˆ—ä¸­çš„æ—§æ•°æ®
            if len(self.raw_data_queue) > 25000:
                # ä¿ç•™æœ€æ–°çš„ä¸€åŠæ•°æ®
                new_queue = deque(maxlen=self.max_queue_size)
                for _ in range(len(self.raw_data_queue) // 2):
                    if self.raw_data_queue:
                        new_queue.append(self.raw_data_queue.pop())
                self.raw_data_queue = new_queue
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
        except Exception:
            pass
    
    def _force_cleanup(self):
        """å¼ºåˆ¶æ¸…ç† - æœ€åæ‰‹æ®µ"""
        try:
            # æ¸…ç©ºé˜Ÿåˆ—
            self.raw_data_queue.clear()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # è®°å½•è­¦å‘Šï¼ˆä½†ä¸å½±å“æ€§èƒ½ï¼‰
            if time.time() - self.stats.get('last_warning', 0) > 60:
                logger.warning("âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œæ‰§è¡Œå¼ºåˆ¶æ¸…ç†")
                self.stats['last_warning'] = time.time()
            
        except Exception:
            pass
    
    def _cleanup_redis_data(self):
        """æ¸…ç†Redisæ•°æ®"""
        try:
            # ä¿æŒRedisé˜Ÿåˆ—åœ¨åˆç†å¤§å°
            self.redis_client.ltrim('stock:raw:queue', 0, 50000)
            
        except Exception:
            pass
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            'received_count': self.stats['received'],
            'stored_count': self.stats['stored'],
            'memory_mb': self.stats['memory_mb'],
            'queue_size': self.stats['queue_size'],
            'memory_usage_percent': (self.stats['memory_mb'] / 100) * 100,
            'is_running': self.running,
            'last_check': self.stats['last_check']
        }
    
    def is_healthy(self) -> bool:
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        return (
            self.running and
            self.stats['memory_mb'] < 80 and
            len(self.raw_data_queue) < self.max_queue_size * 0.8
        )

# å…¨å±€å®ä¾‹
ultra_receiver = UltraHighPerformanceReceiver()

def start_ultra_receiver():
    """å¯åŠ¨è¶…é«˜æ€§èƒ½æ¥æ”¶å™¨"""
    ultra_receiver.start()
    return ultra_receiver

def stop_ultra_receiver():
    """åœæ­¢è¶…é«˜æ€§èƒ½æ¥æ”¶å™¨"""
    ultra_receiver.stop()

def receive_stock_data(raw_data: bytes):
    """æ¥æ”¶è‚¡ç¥¨æ•°æ®çš„å…¥å£å‡½æ•°"""
    ultra_receiver.receive_data(raw_data)

def get_receiver_stats():
    """è·å–æ¥æ”¶å™¨ç»Ÿè®¡"""
    return ultra_receiver.get_stats()

class DataAccumulationMonitor:
    """æ•°æ®å †ç§¯ç›‘æ§å™¨ - é˜²æ­¢è¶…è¿‡100Må¯¼è‡´æœåŠ¡ç«¯æ–­å¼€è¿æ¥"""

    def __init__(self, receiver: UltraHighPerformanceReceiver):
        self.receiver = receiver
        self.max_data_size_mb = 90  # 90MBè­¦æˆ’çº¿
        self.critical_size_mb = 95  # 95MBå±é™©çº¿
        self.monitoring = False
        self.monitor_thread = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_data_size = 0
        self.data_rate_mb_per_sec = 0
        self.last_size_check = 0

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§æ•°æ®å †ç§¯"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        logger.warning("ğŸ” æ•°æ®å †ç§¯ç›‘æ§å™¨å¯åŠ¨")

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5)

    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        last_check_time = time.time()
        last_data_size = 0

        while self.monitoring:
            try:
                current_time = time.time()

                # è®¡ç®—å½“å‰æ•°æ®å¤§å°
                current_data_size = self._calculate_total_data_size()

                # è®¡ç®—æ•°æ®å¢é•¿ç‡
                time_diff = current_time - last_check_time
                if time_diff > 0:
                    size_diff = current_data_size - last_data_size
                    self.data_rate_mb_per_sec = size_diff / time_diff

                # æ£€æŸ¥æ˜¯å¦æ¥è¿‘100Mé™åˆ¶
                if current_data_size > self.critical_size_mb:
                    self._handle_critical_accumulation()
                elif current_data_size > self.max_data_size_mb:
                    self._handle_warning_accumulation()

                # æ›´æ–°ç»Ÿè®¡
                self.total_data_size = current_data_size
                self.last_size_check = current_time

                last_check_time = current_time
                last_data_size = current_data_size

                time.sleep(0.5)  # 500msæ£€æŸ¥ä¸€æ¬¡

            except Exception:
                pass

    def _calculate_total_data_size(self) -> float:
        """è®¡ç®—æ€»æ•°æ®å¤§å°ï¼ˆMBï¼‰"""
        try:
            # å†…å­˜ä¸­çš„æ•°æ®
            memory_mb = self.receiver.stats['memory_mb']

            # Redisä¸­çš„æ•°æ®å¤§å°ä¼°ç®—
            redis_size_mb = 0
            try:
                redis_info = self.receiver.redis_client.info('memory')
                redis_size_mb = redis_info.get('used_memory', 0) / 1024 / 1024
            except Exception:
                pass

            return memory_mb + redis_size_mb * 0.1  # Redisåªç®—10%ï¼Œå› ä¸ºæ˜¯å…±äº«çš„

        except Exception:
            return 0

    def _handle_warning_accumulation(self):
        """å¤„ç†è­¦å‘Šçº§åˆ«çš„æ•°æ®å †ç§¯"""
        try:
            # åŠ é€Ÿæ•°æ®å¤„ç†
            self.receiver._emergency_cleanup()

            # å‡å°‘æ‰¹é‡å¤§å°ï¼ŒåŠ å¿«å¤„ç†é€Ÿåº¦
            self.receiver.redis_batch_size = min(1000, self.receiver.redis_batch_size)

        except Exception:
            pass

    def _handle_critical_accumulation(self):
        """å¤„ç†å±é™©çº§åˆ«çš„æ•°æ®å †ç§¯"""
        try:
            # å¼ºåˆ¶æ¸…ç†
            self.receiver._force_cleanup()

            # è¿›ä¸€æ­¥å‡å°‘æ‰¹é‡å¤§å°
            self.receiver.redis_batch_size = 500

            # æ¸…ç†Redisæ•°æ®
            self.receiver._cleanup_redis_data()

        except Exception:
            pass

if __name__ == '__main__':
    # æµ‹è¯•è¿è¡Œ
    receiver = start_ultra_receiver()

    # å¯åŠ¨æ•°æ®å †ç§¯ç›‘æ§
    monitor = DataAccumulationMonitor(receiver)
    monitor.start_monitoring()

    try:
        # æ¨¡æ‹Ÿæ¥æ”¶5000+è‚¡ç¥¨æ•°æ®
        import random

        print("ğŸš€ å¼€å§‹5000+è‚¡ç¥¨æ•°æ®æ¥æ”¶æµ‹è¯•...")

        for i in range(200000):  # æ¨¡æ‹Ÿ20ä¸‡æ¡æ•°æ®
            # æ¨¡æ‹Ÿä¸åŒè‚¡ç¥¨çš„æ•°æ®
            stock_codes = [f'sz{j:06d}' for j in range(5000)]
            stock_code = random.choice(stock_codes)

            # æ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®
            test_data = f"{stock_code}$è‚¡ç¥¨{i}$12.30$1000000${random.random()}".encode('utf-8')
            receive_stock_data(test_data)

            if i % 20000 == 0:
                stats = get_receiver_stats()
                print(f"ğŸ“Š è¿›åº¦: {i}, å†…å­˜: {stats['memory_mb']:.2f}MB, é˜Ÿåˆ—: {stats['queue_size']}, æ•°æ®ç‡: {monitor.data_rate_mb_per_sec:.2f}MB/s")

        # ç­‰å¾…å¤„ç†å®Œæˆ
        print("â³ ç­‰å¾…æ•°æ®å¤„ç†å®Œæˆ...")
        time.sleep(10)

        final_stats = get_receiver_stats()
        print(f"ğŸ‰ æµ‹è¯•å®Œæˆ: {final_stats}")

    except KeyboardInterrupt:
        print("â¹ï¸ æµ‹è¯•è¢«ä¸­æ–­")
    finally:
        monitor.stop_monitoring()
        stop_ultra_receiver()
