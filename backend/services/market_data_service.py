import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
from typing import List, Dict, Any, Optional, Tuple, Literal

from data_fetchers.tdx_crawler import TdxDataCrawler
from data_fetchers.ths_crawler import ThsCrawler

logger = logging.getLogger(__name__)

# ******************************************************************************
# NOTE: This service is essential for the AI trading system and is actively used
# even though the dedicated "å¸‚åœºè¿½è¸ª" frontend UI has been removed. This service 
# provides market data collection, caching, and access functionality that is
# used by multiple backend components, particularly the AI trading features.
# ******************************************************************************

class MarketDataService:
    """
    å¸‚åœºæ•°æ®æœåŠ¡
    æ•´åˆå¤šä¸ªæ•°æ®æº,æä¾›ç»Ÿä¸€çš„æ¥å£è®¿é—®è‚¡ç¥¨å¸‚åœºæ•°æ®
    """
    
    def __init__(self, tdx_path=None, cache_dir='data/cache'):
        """
        åˆå§‹åŒ–å¸‚åœºæ•°æ®æœåŠ¡
        
        Args:
            tdx_path: é€šè¾¾ä¿¡å®‰è£…è·¯å¾„,å¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨å¯»æ‰¾
            cache_dir: æ•°æ®ç¼“å­˜ç›®å½•
        """
        # åˆå§‹åŒ–æ•°æ®çˆ¬å–å™¨
        self.tdx_crawler = TdxDataCrawler(tdx_path)
        self.ths_crawler = ThsCrawler()
        
        # ç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        # è‚¡ç¥¨åŸºç¡€æ•°æ®ç¼“å­˜
        self.stock_list_cache = None
        self.stock_list_cache_time = None
        
        # æŒ‡æ•°æ•°æ®ç¼“å­˜
        self.index_data_cache = {}
    
    def get_stock_list(self, data_source: str = 'auto', use_cache=True, cache_days=1):
        """
        è·å–è‚¡ç¥¨åˆ—è¡¨
        
        Args:
            data_source: æ•°æ®æº,'tdx'=é€šè¾¾ä¿¡, 'ths'=åŒèŠ±é¡º, 'auto'=è‡ªåŠ¨é€‰æ‹©
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            cache_days: ç¼“å­˜å¤©æ•°,è¶…è¿‡åˆ™é‡æ–°è·å–
            
        Returns:
            pd.DataFrame: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
        """
        cache_file = os.path.join(self.cache_dir, f'stock_list_{data_source}.csv')
        
        # å¦‚æœä½¿ç”¨ç¼“å­˜ä¸”ç¼“å­˜å­˜åœ¨,åˆ™åŠ è½½ç¼“å­˜
        if use_cache:
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            if self.stock_list_cache is not None and self.stock_list_cache_time is not None:
                cache_age = (datetime.now() - self.stock_list_cache_time).days
                if cache_age < cache_days:
                    return self.stock_list_cache
            
            # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
            if os.path.exists(cache_file):
                file_mtime = datetime.fromtimestamp(os.path.getmtime(cache_file))
                file_age = (datetime.now() - file_mtime).days
                
                if file_age < cache_days:
                    try:
                        df = pd.read_csv(cache_file)
                        self.stock_list_cache = df
                        self.stock_list_cache_time = file_mtime
                        return df
                    except Exception as e:
                        logger.warning(f"è¯»å–è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–ç¼“å­˜è¿‡æœŸ,åˆ™é‡æ–°è·å–
        if data_source == 'tdx':
            df = self.tdx_crawler.get_stock_list()
        elif data_source == 'ths':
            df = self.ths_crawler.get_stock_list()
        else:  # auto - è‡ªåŠ¨é€‰æ‹©,ä¼˜å…ˆé€šè¾¾ä¿¡,é€šè¾¾ä¿¡å¤±è´¥åˆ™å°è¯•åŒèŠ±é¡º
            df = self.tdx_crawler.get_stock_list()
            if df.empty:
                df = self.ths_crawler.get_stock_list()
        
        # æ›´æ–°ç¼“å­˜
        if not df.empty:
            self.stock_list_cache = df
            self.stock_list_cache_time = datetime.now()
            
            try:
                df.to_csv(cache_file, index=False)
            except Exception as e:
                logger.warning(f"ä¿å­˜è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜å¤±è´¥: {e}")
        
        return df
    
    def get_k_data(self, code, start_date=None, end_date=None, freq='daily', 
                   data_source: str = 'auto', use_cache=True, cache_days=1):
        """
        è·å–è‚¡ç¥¨Kçº¿æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ,æ ¼å¼:'YYYY-MM-DD'
            end_date: ç»“æŸæ—¥æœŸ,æ ¼å¼:'YYYY-MM-DD'
            freq: æ•°æ®é¢‘ç‡,æ”¯æŒ 'daily', 'weekly', 'monthly'
            data_source: æ•°æ®æº,'tdx'=é€šè¾¾ä¿¡, 'ths'=åŒèŠ±é¡º, 'auto'=è‡ªåŠ¨é€‰æ‹©
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            cache_days: ç¼“å­˜å¤©æ•°,è¶…è¿‡åˆ™é‡æ–°è·å–
            
        Returns:
            pd.DataFrame: Kçº¿æ•°æ®
        """
        if not start_date:
            start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')
        if not end_date:
            end_date = datetime.now().strftime('%Y-%m-%d')
            
        # ç¼“å­˜æ–‡ä»¶å‘½å
        cache_file = os.path.join(self.cache_dir, f"{code}_{freq}_{start_date}_{end_date}_{data_source}.csv")
        
        # å¦‚æœä½¿ç”¨ç¼“å­˜ä¸”ç¼“å­˜å­˜åœ¨,åˆ™åŠ è½½ç¼“å­˜
        if use_cache and os.path.exists(cache_file):
            file_mtime = datetime.fromtimestamp(os.path.getmtime(cache_file))
            file_age = (datetime.now() - file_mtime).days
            
            if file_age < cache_days:
                try:
                    df = pd.read_csv(cache_file)
                    # è½¬æ¢æ—¥æœŸåˆ—
                    if 'date' in df.columns:
                        df['date'] = pd.to_datetime(df['date'])
                    return df
                except Exception as e:
                    logger.warning(f"è¯»å–Kçº¿æ•°æ®ç¼“å­˜å¤±è´¥ - è‚¡ç¥¨: {code}, é”™è¯¯: {e}")
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–ç¼“å­˜è¿‡æœŸ,åˆ™é‡æ–°è·å–
        if data_source == 'tdx':
            df = self.tdx_crawler.get_k_data(code, start_date, end_date, freq)
        elif data_source == 'ths':
            df = self.ths_crawler.get_k_data(code, start_date, end_date, freq)
        else:  # auto - è‡ªåŠ¨é€‰æ‹©
            df = self.tdx_crawler.get_k_data(code, start_date, end_date, freq)
            if df.empty:
                df = self.ths_crawler.get_k_data(code, start_date, end_date, freq)
        
        # æ›´æ–°ç¼“å­˜
        if not df.empty:
            try:
                df.to_csv(cache_file, index=False)
            except Exception as e:
                logger.warning(f"ä¿å­˜Kçº¿æ•°æ®ç¼“å­˜å¤±è´¥ - è‚¡ç¥¨: {code}, é”™è¯¯: {e}")
        
        return df
    
    def get_realtime_quotes(self, codes, data_source: str = 'auto'):
        """
        è·å–å®æ—¶è¡Œæƒ…æ•°æ®,ä¸ç¼“å­˜
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨,å¦‚ ['000001', '600000']
            data_source: æ•°æ®æº,'tdx'=é€šè¾¾ä¿¡, 'ths'=åŒèŠ±é¡º, 'auto'=è‡ªåŠ¨é€‰æ‹©
            
        Returns:
            pd.DataFrame: å®æ—¶è¡Œæƒ…æ•°æ®
        """
        if data_source == 'tdx':
            return self.tdx_crawler.get_realtime_quotes(codes)
        elif data_source == 'ths':
            return self.ths_crawler.get_realtime_quotes(codes)
        else:  # auto - è‡ªåŠ¨é€‰æ‹©
            df = self.tdx_crawler.get_realtime_quotes(codes)
            if df.empty:
                df = self.ths_crawler.get_realtime_quotes(codes)
            return df
    
    def get_index_data(self, index_code, start_date=None, end_date=None, 
                       data_source: str = 'auto', use_cache=True, cache_days=1):
        """
        è·å–æŒ‡æ•°æ•°æ®
        
        Args:
            index_code: æŒ‡æ•°ä»£ç ,å¦‚ '000001' (ä¸Šè¯æŒ‡æ•°)
            start_date: å¼€å§‹æ—¥æœŸ,æ ¼å¼:'YYYY-MM-DD'
            end_date: ç»“æŸæ—¥æœŸ,æ ¼å¼:'YYYY-MM-DD'
            data_source: æ•°æ®æº,'tdx'=é€šè¾¾ä¿¡, 'ths'=åŒèŠ±é¡º, 'auto'=è‡ªåŠ¨é€‰æ‹©
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            cache_days: ç¼“å­˜å¤©æ•°,è¶…è¿‡åˆ™é‡æ–°è·å–
            
        Returns:
            pd.DataFrame: æŒ‡æ•°æ•°æ®
        """
        if not start_date:
            start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')
        if not end_date:
            end_date = datetime.now().strftime('%Y-%m-%d')
            
        # ç¼“å­˜æ–‡ä»¶å‘½å
        cache_file = os.path.join(self.cache_dir, f"index_{index_code}_{start_date}_{end_date}_{data_source}.csv")
        
        # å¦‚æœä½¿ç”¨ç¼“å­˜ä¸”ç¼“å­˜å­˜åœ¨,åˆ™åŠ è½½ç¼“å­˜
        if use_cache:
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            cache_key = f"{index_code}_{start_date}_{end_date}_{data_source}"
            if cache_key in self.index_data_cache:
                cache_time, df = self.index_data_cache[cache_key]
                cache_age = (datetime.now() - cache_time).days
                if cache_age < cache_days:
                    return df.copy()
            
            # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
            if os.path.exists(cache_file):
                file_mtime = datetime.fromtimestamp(os.path.getmtime(cache_file))
                file_age = (datetime.now() - file_mtime).days
                
                if file_age < cache_days:
                    try:
                        df = pd.read_csv(cache_file)
                        # è½¬æ¢æ—¥æœŸåˆ—
                        if 'date' in df.columns:
                            df['date'] = pd.to_datetime(df['date'])
                        # æ›´æ–°å†…å­˜ç¼“å­˜
                        self.index_data_cache[cache_key] = (file_mtime, df)
                        return df
                    except Exception as e:
                        logger.warning(f"è¯»å–æŒ‡æ•°æ•°æ®ç¼“å­˜å¤±è´¥ - æŒ‡æ•°: {index_code}, é”™è¯¯: {e}")
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–ç¼“å­˜è¿‡æœŸ,åˆ™é‡æ–°è·å–
        if data_source == 'tdx':
            df = self.tdx_crawler.get_index_data(index_code, start_date, end_date)
        elif data_source == 'ths':
            df = self.ths_crawler.get_index_data(index_code, start_date, end_date)
        else:  # auto - è‡ªåŠ¨é€‰æ‹©
            df = self.tdx_crawler.get_index_data(index_code, start_date, end_date)
            if df.empty:
                df = self.ths_crawler.get_index_data(index_code, start_date, end_date)
        
        # æ›´æ–°ç¼“å­˜
        if not df.empty:
            # æ›´æ–°å†…å­˜ç¼“å­˜
            cache_key = f"{index_code}_{start_date}_{end_date}_{data_source}"
            self.index_data_cache[cache_key] = (datetime.now(), df.copy())
            
            try:
                df.to_csv(cache_file, index=False)
            except Exception as e:
                logger.warning(f"ä¿å­˜æŒ‡æ•°æ•°æ®ç¼“å­˜å¤±è´¥ - æŒ‡æ•°: {index_code}, é”™è¯¯: {e}")
        
        return df
    
    def get_stock_fundamentals(self, code, data_source: str = 'tdx'):
        """
        è·å–è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            data_source: æ•°æ®æº,'tdx'=é€šè¾¾ä¿¡, 'ths'=åŒèŠ±é¡º, 'auto'=è‡ªåŠ¨é€‰æ‹©
            
        Returns:
            Dict: è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®
        """
        # åŸºæœ¬é¢æ•°æ®ç›®å‰åªé€šè¿‡é€šè¾¾ä¿¡è·å–
        return self.tdx_crawler.get_stock_fundamentals(code)
    
    def get_main_finance_indicators(self, code, data_source: str = 'tdx'):
        """
        è·å–ä¸»è¦è´¢åŠ¡æŒ‡æ ‡
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            data_source: æ•°æ®æº,'tdx'=é€šè¾¾ä¿¡, 'ths'=åŒèŠ±é¡º, 'auto'=è‡ªåŠ¨é€‰æ‹©
            
        Returns:
            pd.DataFrame: ä¸»è¦è´¢åŠ¡æŒ‡æ ‡
        """
        # è´¢åŠ¡æŒ‡æ ‡ç›®å‰åªé€šè¿‡é€šè¾¾ä¿¡è·å–
        return self.tdx_crawler.get_main_finance_indicators(code)
    
    def get_industry_stocks(self, industry, data_source: str = 'auto', use_cache=True):
        """
        è·å–æŒ‡å®šè¡Œä¸šçš„è‚¡ç¥¨åˆ—è¡¨
        
        Args:
            industry: è¡Œä¸šåç§°,å¦‚ 'é“¶è¡Œ', 'è¯åˆ¸' ç­‰
            data_source: æ•°æ®æº,'tdx'=é€šè¾¾ä¿¡, 'ths'=åŒèŠ±é¡º, 'auto'=è‡ªåŠ¨é€‰æ‹©
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            pd.DataFrame: æŒ‡å®šè¡Œä¸šçš„è‚¡ç¥¨åˆ—è¡¨
        """
        # è·å–è‚¡ç¥¨åˆ—è¡¨,ä½¿ç”¨ç›¸åŒçš„ç¼“å­˜è®¾ç½®
        stock_list = self.get_stock_list(data_source=data_source, use_cache=use_cache)
        
        if data_source == 'tdx':
            return self.tdx_crawler.get_industry_stocks(industry)
        elif data_source == 'ths':
            # åŒèŠ±é¡ºæš‚ä¸æ”¯æŒè¡Œä¸šè‚¡ç¥¨è·å–,ä½¿ç”¨é€šè¾¾ä¿¡
            return self.tdx_crawler.get_industry_stocks(industry)
        else:  # auto
            return self.tdx_crawler.get_industry_stocks(industry)
    
    def merge_data_sources(self, code, start_date=None, end_date=None, freq='daily'):
        """
        åˆå¹¶å¤šä¸ªæ•°æ®æºçš„æ•°æ®,è·å–æ›´å®Œæ•´çš„æ•°æ®é›†
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ,æ ¼å¼:'YYYY-MM-DD'
            end_date: ç»“æŸæ—¥æœŸ,æ ¼å¼:'YYYY-MM-DD'
            freq: æ•°æ®é¢‘ç‡,æ”¯æŒ 'daily', 'weekly', 'monthly'
            
        Returns:
            pd.DataFrame: åˆå¹¶åçš„Kçº¿æ•°æ®
        """
        # ä»å¤šä¸ªæ•°æ®æºè·å–æ•°æ®
        tdx_data = self.get_k_data(code, start_date, end_date, freq, data_source='tdx')
        ths_data = self.get_k_data(code, start_date, end_date, freq, data_source='ths')
        
        # å¦‚æœå…¶ä¸­ä¸€ä¸ªä¸ºç©º,åˆ™ç›´æ¥è¿”å›å¦ä¸€ä¸ª
        if tdx_data.empty:
            return ths_data
        if ths_data.empty:
            return tdx_data
        
        # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºæ—¥æœŸç±»å‹
        if 'date' in tdx_data.columns and not pd.api.types.is_datetime64_any_dtype(tdx_data['date']):
            tdx_data['date'] = pd.to_datetime(tdx_data['date'])
        if 'date' in ths_data.columns and not pd.api.types.is_datetime64_any_dtype(ths_data['date']):
            ths_data['date'] = pd.to_datetime(ths_data['date'])
        
        # ä»¥æ—¥æœŸä¸ºç´¢å¼•åˆå¹¶æ•°æ®
        tdx_data.set_index('date', inplace=True)
        ths_data.set_index('date', inplace=True)
        
        # åˆå¹¶æ•°æ®,ä¼˜å…ˆä½¿ç”¨é€šè¾¾ä¿¡æ•°æ®,ç¼ºå¤±åˆ™ä½¿ç”¨åŒèŠ±é¡ºæ•°æ®
        merged_data = tdx_data.combine_first(ths_data)
        
        # é‡ç½®ç´¢å¼•
        merged_data.reset_index(inplace=True)
        
        # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
        merged_data.sort_values('date', inplace=True)
        
        return merged_data
    
    def clear_cache(self, older_than_days=None, pattern=None, data_source=None):
        """
        æ¸…é™¤ç¼“å­˜æ–‡ä»¶
        
        Args:
            older_than_days: æ¸…é™¤æ—©äºæŒ‡å®šå¤©æ•°çš„ç¼“å­˜
            pattern: æ–‡ä»¶ååŒ¹é…æ¨¡å¼,å¦‚ 'index_*.csv'
            data_source: æŒ‡å®šæ•°æ®æºçš„ç¼“å­˜,å¦‚ 'tdx', 'ths'
            
        Returns:
            int: æ¸…é™¤çš„ç¼“å­˜æ–‡ä»¶æ•°é‡
        """
        import glob
        
        if pattern and data_source:
            files = glob.glob(os.path.join(self.cache_dir, f"{pattern}_{data_source}.csv"))
        elif pattern:
            files = glob.glob(os.path.join(self.cache_dir, f"{pattern}.csv"))
        elif data_source:
            files = glob.glob(os.path.join(self.cache_dir, f"*_{data_source}.csv"))
        else:
            files = glob.glob(os.path.join(self.cache_dir, '*.csv'))
            
        now = datetime.now()
        count = 0
        
        for file in files:
            delete = True
            
            if older_than_days is not None:
                file_mtime = datetime.fromtimestamp(os.path.getmtime(file))
                file_age = (now - file_mtime).days
                
                if file_age < older_than_days:
                    delete = False
                    
            if delete:
                try:
                    os.remove(file)
                    count += 1
                except Exception as e:
                    logger.error(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {file}, é”™è¯¯: {e}")
                    
        return count
    
    def get_data_source_delay(self, source: str) -> int:
        """
        è·å–æ•°æ®æºçš„è¿æ¥å»¶è¿Ÿ(æ¯«ç§’)
        
        Args:
            source: æ•°æ®æºåç§°,"tdx"=é€šè¾¾ä¿¡,"ths"=åŒèŠ±é¡º
            
        Returns:
            int: è¿æ¥å»¶è¿Ÿå€¼(æ¯«ç§’),å¦‚æœæ— æ³•æµ‹é‡åˆ™è¿”å›None
        """
        try:
            import random
            import time
            
            # å®é™…ç¯å¢ƒä¸­,åº”è¯¥æµ‹é‡APIè°ƒç”¨æ—¶é—´
            # ä»¥ä¸‹ä»£ç æ¼”ç¤ºå¦‚ä½•æµ‹é‡å®é™…å»¶è¿Ÿ
            if source == "tdx":
                # ä¸ºé¿å…è¿‡åº¦è¯·æ±‚API,ä½¿ç”¨ç¼“å­˜ç»“æœå’Œéšæœºå˜åŒ–æ¨¡æ‹ŸçœŸå®æƒ…å†µ
                # å®é™…åº”ç”¨ä¸­åº”æµ‹é‡å®é™…è¯·æ±‚æ—¶é—´
                start_time = time.time()
                try:
                    # å°è¯•å®é™…è¯·æ±‚
                    _ = self.tdx_crawler.get_realtime_quotes(['000001'])
                    end_time = time.time()
                    actual_delay = int((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                    
                    # å¦‚æœè¯·æ±‚æˆåŠŸ,ä½¿ç”¨å®é™…æµ‹é‡çš„å»¶è¿Ÿ
                    # æ·»åŠ å°‘é‡éšæœºæ³¢åŠ¨æ¨¡æ‹Ÿç½‘ç»œå˜åŒ–
                    return actual_delay + random.randint(-50, 50)
                except Exception:
                    # å¦‚æœè¯·æ±‚å¤±è´¥,ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                    return random.randint(100, 600)
                    
            elif source == "ths":
                # åŒç†,æµ‹é‡åŒèŠ±é¡ºAPIå»¶è¿Ÿ
                start_time = time.time()
                try:
                    # å°è¯•å®é™…è¯·æ±‚
                    _ = self.ths_crawler.get_realtime_quotes(['000001'])
                    end_time = time.time()
                    actual_delay = int((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                    
                    # å¦‚æœè¯·æ±‚æˆåŠŸ,ä½¿ç”¨å®é™…æµ‹é‡çš„å»¶è¿Ÿ
                    # æ·»åŠ å°‘é‡éšæœºæ³¢åŠ¨æ¨¡æ‹Ÿç½‘ç»œå˜åŒ–
                    return actual_delay + random.randint(-50, 50)
                except Exception:
                    # å¦‚æœè¯·æ±‚å¤±è´¥,ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                    return random.randint(150, 800)
            
            return None
        except Exception as e:
            logger.error(f"è·å–æ•°æ®æº{source}å»¶è¿Ÿä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    def get_market_indices(self) -> List[Dict[str, Any]]:
        """
        è·å–ä¸»è¦å¸‚åœºæŒ‡æ•°è¡Œæƒ…
        
        Returns:
            List[Dict]: æŒ‡æ•°æ•°æ®åˆ—è¡¨
        """
        try:
            # è·å–å®æ—¶è¡Œæƒ…
            codes = ['000001', '399001', '399006', '000300'] # ä¸Šè¯æŒ‡æ•°,æ·±è¯æˆæŒ‡,åˆ›ä¸šæ¿æŒ‡,æ²ªæ·±300
            indices_data = self.get_realtime_quotes(codes, data_source='auto')
            
            if indices_data.empty:
                # å¦‚æœæ— æ³•è·å–å®æ—¶æ•°æ®,è¿”å›æ¨¡æ‹Ÿæ•°æ®
                return [
                    {"name": "ä¸Šè¯æŒ‡æ•°", "code": "000001", "price": 3458.23, "change": 1.35},
                    {"name": "æ·±è¯æˆæŒ‡", "code": "399001", "price": 14256.89, "change": 1.62},
                    {"name": "åˆ›ä¸šæ¿æŒ‡", "code": "399006", "price": 2876.45, "change": -0.32},
                    {"name": "æ²ªæ·±300", "code": "000300", "price": 4652.78, "change": 1.18}
                ]
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            result = []
            for _, row in indices_data.iterrows():
                result.append({
                    "name": row.get('name', ''),
                    "code": row.get('code', ''),
                    "price": row.get('price', 0.0),
                    "change": row.get('change_pct', 0.0)
                })
            
            return result
        except Exception as e:
            logger.error(f"è·å–å¸‚åœºæŒ‡æ•°æ•°æ®å¤±è´¥: {str(e)}")
            # è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return [
                {"name": "ä¸Šè¯æŒ‡æ•°", "code": "000001", "price": 3458.23, "change": 1.35},
                {"name": "æ·±è¯æˆæŒ‡", "code": "399001", "price": 14256.89, "change": 1.62},
                {"name": "åˆ›ä¸šæ¿æŒ‡", "code": "399006", "price": 2876.45, "change": -0.32},
                {"name": "æ²ªæ·±300", "code": "000300", "price": 4652.78, "change": 1.18}
            ]
    
    def get_sector_performance(self) -> List[Dict[str, Any]]:
        """
        è·å–è¡Œä¸šæ¿å—è¡¨ç°
        
        Returns:
            List[Dict]: è¡Œä¸šæ¿å—æ•°æ®åˆ—è¡¨
        """
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ç›¸åº”APIè·å–è¡Œä¸šæ¿å—æ•°æ®
            # å‡ºäºæ¼”ç¤ºç›®çš„,è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return [
                {"name": "é£Ÿå“é¥®æ–™", "change": 2.15, "strength": 8},
                {"name": "é“¶è¡Œ", "change": 0.87, "strength": 6},
                {"name": "åŒ»è¯ç”Ÿç‰©", "change": -0.35, "strength": 4},
                {"name": "æ–°èƒ½æº", "change": -1.25, "strength": 3},
                {"name": "ç”µå­ç§‘æŠ€", "change": 1.48, "strength": 7}
            ]
        except Exception as e:
            logger.error(f"è·å–è¡Œä¸šæ¿å—æ•°æ®å¤±è´¥: {str(e)}")
            return []
    
    def is_using_simulated_data(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ­£åœ¨ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        ğŸš¨ ç³»ç»Ÿå·²ç¦ç”¨æ‰€æœ‰æ¨¡æ‹Ÿæ•°æ®ï¼Œåªå…è®¸çœŸå®æ•°æ®æº

        Returns:
            bool: å¦‚æœæ— æ³•è·å–çœŸå®æ•°æ®è¿”å›Trueï¼Œè¡¨ç¤ºæ•°æ®ä¸å¯ç”¨
        """
        try:
            # æ£€æŸ¥æ·˜å®è‚¡ç¥¨æ•°æ®æ¨é€æœåŠ¡
            if self._check_taobao_data_service():
                return False

            # æ£€æŸ¥é€šè¾¾ä¿¡æ•°æ®æº
            test_code = '000001'  # ä¸Šè¯æŒ‡æ•°
            test_data = self.tdx_crawler.get_realtime_quotes([test_code])
            if not test_data.empty:
                return False

            # æ£€æŸ¥åŒèŠ±é¡ºæ•°æ®æº
            test_data = self.ths_crawler.get_realtime_quotes([test_code])
            if not test_data.empty:
                return False

            # æ‰€æœ‰çœŸå®æ•°æ®æºéƒ½æ— æ³•è·å–æ•°æ®
            logger.error("âŒ æ‰€æœ‰çœŸå®æ•°æ®æºéƒ½ä¸å¯ç”¨ï¼Œæ‹’ç»æä¾›æ¨¡æ‹Ÿæ•°æ®")
            return True
        except Exception as e:
            logger.error(f"âŒ çœŸå®æ•°æ®æºæ£€æŸ¥å¤±è´¥: {e}")
            return True

    def _check_taobao_data_service(self) -> bool:
        """æ£€æŸ¥æ·˜å®è‚¡ç¥¨æ•°æ®æ¨é€æœåŠ¡æ˜¯å¦å¯ç”¨"""
        # TODO: å®ç°æ·˜å®è‚¡ç¥¨æ•°æ®æ¨é€æœåŠ¡æ£€æŸ¥
        # API_KEY = "QT_wat5QfcJ6N9pDZM5"
        logger.warning("âš ï¸ æ·˜å®è‚¡ç¥¨æ•°æ®æ¨é€æœåŠ¡å°šæœªé…ç½®")
        return False
