#!/usr/bin/env python3
"""
4000+è‚¡ç¥¨æ¨é€æ•°æ®åº“å¤„ç†å™¨
ä¸“é—¨å¤„ç†å¼€ç›˜æ—¶é—´å¤§é‡è‚¡ç¥¨æ•°æ®æ¨é€åˆ°æ•°æ®åº“çš„é«˜æ€§èƒ½ç³»ç»Ÿ
"""

import asyncio
import aiohttp
import json
import os
import time
import pickle
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any
import logging
from concurrent.futures import ThreadPoolExecutor
import queue
import threading

# é…ç½®
SUPABASE_URL = 'https://zzukfxwavknskqcepsjb.supabase.co'
SUPABASE_KEY = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Inp6dWtmeHdhdmtuc2txY2Vwc2piIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTEyOTg1MDYsImV4cCI6MjA2Njg3NDUwNn0.AMGkJSre3QtRBQK_Lh2Iga4dUzSPvuO1G9s6fF2QPaw'

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MassStockDatabaseProcessor:
    def __init__(self):
        self.data_dir = Path('stock_data')
        self.batch_size = 500  # æ¯æ‰¹å¤„ç†500åªè‚¡ç¥¨
        self.max_concurrent = 10  # æœ€å¤§å¹¶å‘æ•°
        self.processed_count = 0
        self.error_count = 0
        self.start_time = None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'total_stocks': 0,
            'processed_stocks': 0,
            'database_writes': 0,
            'errors': 0,
            'start_time': None,
            'end_time': None
        }
        
        # æ•°æ®é˜Ÿåˆ—
        self.data_queue = queue.Queue(maxsize=10000)
        self.result_queue = queue.Queue()
        
    async def process_mass_stock_data(self):
        """å¤„ç†å¤§é‡è‚¡ç¥¨æ¨é€æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹å¤„ç†4000+è‚¡ç¥¨æ¨é€æ•°æ®...")
        self.stats['start_time'] = time.time()
        
        try:
            # 1. æ‰«ææ‰€æœ‰æ¨é€æ•°æ®æ–‡ä»¶
            files = await self.scan_push_data_files()
            self.stats['total_files'] = len(files)
            
            if not files:
                logger.info("ğŸ“ æ²¡æœ‰æ‰¾åˆ°æ¨é€æ•°æ®æ–‡ä»¶")
                return True
            
            logger.info(f"ğŸ“Š æ‰¾åˆ° {len(files)} ä¸ªæ¨é€æ•°æ®æ–‡ä»¶")
            
            # 2. å¹¶è¡Œå¤„ç†æ–‡ä»¶
            await self.process_files_parallel(files)
            
            # 3. ç”Ÿæˆå¤„ç†æŠ¥å‘Š
            await self.generate_processing_report()
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
            return False
        finally:
            self.stats['end_time'] = time.time()
    
    async def scan_push_data_files(self):
        """æ‰«ææ¨é€æ•°æ®æ–‡ä»¶"""
        files = []
        
        if not self.data_dir.exists():
            logger.warning(f"ğŸ“ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return files
        
        # æ‰«æ.datæ–‡ä»¶
        dat_files = list(self.data_dir.glob('*.dat'))
        pkl_files = list(self.data_dir.glob('*.pkl'))
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œä¼˜å…ˆå¤„ç†æœ€æ–°çš„æ–‡ä»¶
        all_files = dat_files + pkl_files
        all_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
        
        logger.info(f"ğŸ“„ æ‰¾åˆ° {len(dat_files)} ä¸ª.datæ–‡ä»¶, {len(pkl_files)} ä¸ª.pklæ–‡ä»¶")
        
        return all_files
    
    async def process_files_parallel(self, files: List[Path]):
        """å¹¶è¡Œå¤„ç†æ–‡ä»¶"""
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def process_single_file(file_path):
            async with semaphore:
                return await self.process_single_file(file_path)
        
        # åˆ›å»ºä»»åŠ¡
        tasks = [process_single_file(file_path) for file_path in files]
        
        # å¹¶è¡Œæ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r is True)
        error_count = len(results) - success_count
        
        logger.info(f"ğŸ“Š æ–‡ä»¶å¤„ç†å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {error_count}")
    
    async def process_single_file(self, file_path: Path):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
            
            # è¯»å–æ–‡ä»¶æ•°æ®
            stock_data_list = await self.read_file_data(file_path)
            
            if not stock_data_list:
                logger.warning(f"âš ï¸ æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯: {file_path.name}")
                return True
            
            # æ‰¹é‡å¤„ç†è‚¡ç¥¨æ•°æ®
            await self.process_stock_batch(stock_data_list, file_path.name)
            
            self.stats['processed_files'] += 1
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
            self.stats['errors'] += 1
            return False
    
    async def read_file_data(self, file_path: Path) -> List[Dict]:
        """è¯»å–æ–‡ä»¶æ•°æ®"""
        try:
            if file_path.suffix == '.dat':
                # JSONæ ¼å¼æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                    if isinstance(data, dict):
                        return [data]
                    elif isinstance(data, list):
                        return data
                    else:
                        return []
            
            elif file_path.suffix == '.pkl':
                # Pickleæ ¼å¼æ–‡ä»¶
                with open(file_path, 'rb') as f:
                    data = pickle.load(f)
                    if isinstance(data, dict):
                        return [data]
                    elif isinstance(data, list):
                        return data
                    else:
                        return []
            
            else:
                logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
                return []
                
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
            return []
    
    async def process_stock_batch(self, stock_data_list: List[Dict], source_file: str):
        """æ‰¹é‡å¤„ç†è‚¡ç¥¨æ•°æ®"""
        try:
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(stock_data_list), self.batch_size):
                batch = stock_data_list[i:i + self.batch_size]
                
                # è½¬æ¢æ•°æ®æ ¼å¼
                db_records = self.convert_to_db_format(batch, source_file)
                
                # å†™å…¥æ•°æ®åº“
                success = await self.batch_write_to_database(db_records)
                
                if success:
                    self.stats['processed_stocks'] += len(batch)
                    self.stats['database_writes'] += 1
                    logger.info(f"âœ… æ‰¹é‡å†™å…¥æˆåŠŸ: {len(batch)} æ¡è®°å½• (æ¥æº: {source_file})")
                else:
                    self.stats['errors'] += 1
                    logger.error(f"âŒ æ‰¹é‡å†™å…¥å¤±è´¥: {len(batch)} æ¡è®°å½•")
                
                # é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                await asyncio.sleep(0.1)
                
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            self.stats['errors'] += 1
    
    def convert_to_db_format(self, stock_data_list: List[Dict], source_file: str) -> List[Dict]:
        """è½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼"""
        db_records = []
        
        for stock_data in stock_data_list:
            try:
                # ä½¿ç”¨system_configè¡¨å­˜å‚¨ï¼Œå› ä¸ºä¸“é—¨çš„è¡¨ä¸å­˜åœ¨
                record = {
                    'key': f'mass_stock_{stock_data.get("symbol", "unknown")}_{int(time.time())}',
                    'value': json.dumps({
                        'symbol': stock_data.get('symbol', ''),
                        'stock_name': stock_data.get('name', ''),
                        'price': float(stock_data.get('price', 0)),
                        'volume': int(stock_data.get('volume', 0)),
                        'change': float(stock_data.get('change', 0)),
                        'change_percent': float(stock_data.get('change_percent', 0)),
                        'timestamp': stock_data.get('timestamp', time.time()),
                        'source_file': source_file,
                        'processed_at': datetime.now().isoformat(),
                        'data_type': 'mass_push_data'
                    }),
                    'description': f'å¤§é‡æ¨é€æ•°æ® - {stock_data.get("symbol", "unknown")}'
                }
                
                db_records.append(record)
                
            except Exception as e:
                logger.error(f"âŒ è½¬æ¢æ•°æ®æ ¼å¼å¤±è´¥: {e}")
                continue
        
        return db_records
    
    async def batch_write_to_database(self, db_records: List[Dict]) -> bool:
        """æ‰¹é‡å†™å…¥æ•°æ®åº“"""
        try:
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {SUPABASE_KEY}',
                'apikey': SUPABASE_KEY,
                'Prefer': 'return=minimal'  # å‡å°‘è¿”å›æ•°æ®é‡
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f'{SUPABASE_URL}/rest/v1/system_config',
                    headers=headers,
                    json=db_records,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    if response.status in [200, 201]:
                        return True
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ æ•°æ®åº“å†™å…¥å¤±è´¥: {response.status} - {error_text}")
                        return False
                        
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“å†™å…¥å¼‚å¸¸: {e}")
            return False
    
    async def generate_processing_report(self):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        duration = self.stats['end_time'] - self.stats['start_time']
        
        report = f"""
ğŸš€ 4000+è‚¡ç¥¨æ¨é€æ•°æ®å¤„ç†æŠ¥å‘Š
{'=' * 50}

ğŸ“Š å¤„ç†ç»Ÿè®¡:
  - æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}
  - å·²å¤„ç†æ–‡ä»¶: {self.stats['processed_files']}
  - æ€»è‚¡ç¥¨æ•°: {self.stats['total_stocks']}
  - å·²å¤„ç†è‚¡ç¥¨: {self.stats['processed_stocks']}
  - æ•°æ®åº“å†™å…¥æ¬¡æ•°: {self.stats['database_writes']}
  - é”™è¯¯æ¬¡æ•°: {self.stats['errors']}

â±ï¸ æ€§èƒ½æŒ‡æ ‡:
  - æ€»è€—æ—¶: {duration:.2f} ç§’
  - å¤„ç†é€Ÿåº¦: {self.stats['processed_stocks'] / duration:.2f} è‚¡ç¥¨/ç§’
  - æˆåŠŸç‡: {(self.stats['processed_stocks'] / max(self.stats['total_stocks'], 1)) * 100:.2f}%

ğŸ¯ ç³»ç»ŸçŠ¶æ€:
  - æ‰¹é‡å¤§å°: {self.batch_size}
  - æœ€å¤§å¹¶å‘: {self.max_concurrent}
  - æ•°æ®åº“: Supabase (system_configè¡¨)
  - å­˜å‚¨æ ¼å¼: JSON in system_config.value

{'=' * 50}
"""
        
        logger.info(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = f'mass_stock_processing_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ğŸ“„ å¤„ç†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    async def cleanup_old_files(self, days_old: int = 1):
        """æ¸…ç†æ—§æ–‡ä»¶"""
        try:
            cutoff_time = datetime.now() - timedelta(days=days_old)
            cleaned_count = 0
            
            for file_path in self.data_dir.iterdir():
                if file_path.is_file():
                    file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if file_time < cutoff_time:
                        file_path.unlink()
                        cleaned_count += 1
            
            if cleaned_count > 0:
                logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} ä¸ªæ—§æ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ–‡ä»¶å¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    processor = MassStockDatabaseProcessor()
    
    logger.info("ğŸš€ å¯åŠ¨4000+è‚¡ç¥¨æ¨é€æ•°æ®åº“å¤„ç†å™¨...")
    
    # å¤„ç†å¤§é‡è‚¡ç¥¨æ•°æ®
    success = await processor.process_mass_stock_data()
    
    if success:
        logger.info("ğŸ‰ å¤§é‡è‚¡ç¥¨æ•°æ®å¤„ç†å®Œæˆï¼")
        
        # æ¸…ç†æ—§æ–‡ä»¶
        await processor.cleanup_old_files()
        
    else:
        logger.error("âŒ å¤§é‡è‚¡ç¥¨æ•°æ®å¤„ç†å¤±è´¥")
    
    return success

if __name__ == '__main__':
    asyncio.run(main())
