#!/usr/bin/env python3
"""
å¼€ç›˜æ—¶é—´4000+è‚¡ç¥¨æ¨é€ç›‘æ§å™¨
ä¸“é—¨ç›‘æ§å’Œå¤„ç†å¼€ç›˜æ—¶é—´çš„å¤§é‡è‚¡ç¥¨æ•°æ®æ¨é€
"""

import asyncio
import time
import json
import os
from datetime import datetime, timedelta
from pathlib import Path
import logging
import requests
from typing import Dict, List, Any

# é…ç½®
SUPABASE_URL = 'https://zzukfxwavknskqcepsjb.supabase.co'
SUPABASE_KEY = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Inp6dWtmeHdhdmtuc2txY2Vwc2piIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTEyOTg1MDYsImV4cCI6MjA2Njg3NDUwNn0.AMGkJSre3QtRBQK_Lh2Iga4dUzSPvuO1G9s6fF2QPaw'

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OpeningTimeMonitor:
    def __init__(self):
        self.data_dir = Path('stock_data')
        self.monitoring = False
        self.stats = {
            'monitoring_start': None,
            'total_files_detected': 0,
            'total_stocks_processed': 0,
            'database_writes': 0,
            'errors': 0,
            'peak_files_per_minute': 0,
            'peak_stocks_per_minute': 0
        }
        
        # å¼€ç›˜æ—¶é—´é…ç½®
        self.market_open_times = {
            'morning_open': '09:30',
            'morning_close': '11:30',
            'afternoon_open': '13:00',
            'afternoon_close': '15:00'
        }
        
    def is_market_open(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åœ¨å¼€ç›˜æ—¶é—´"""
        now = datetime.now()
        current_time = now.strftime('%H:%M')
        
        # æ£€æŸ¥æ˜¯å¦åœ¨äº¤æ˜“æ—¶é—´å†…
        morning_open = self.market_open_times['morning_open']
        morning_close = self.market_open_times['morning_close']
        afternoon_open = self.market_open_times['afternoon_open']
        afternoon_close = self.market_open_times['afternoon_close']
        
        is_morning_session = morning_open <= current_time <= morning_close
        is_afternoon_session = afternoon_open <= current_time <= afternoon_close
        
        return is_morning_session or is_afternoon_session
    
    def is_opening_time(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å¼€ç›˜æ—¶é—´ï¼ˆå‰5åˆ†é’Ÿï¼‰"""
        now = datetime.now()
        current_time = now.strftime('%H:%M')
        
        # å¼€ç›˜å‰5åˆ†é’Ÿåˆ°å¼€ç›˜å5åˆ†é’Ÿ
        opening_times = ['09:25', '09:35', '12:55', '13:05']
        
        for open_time in opening_times:
            open_dt = datetime.strptime(open_time, '%H:%M').replace(
                year=now.year, month=now.month, day=now.day
            )
            time_diff = abs((now - open_dt).total_seconds())
            
            if time_diff <= 300:  # 5åˆ†é’Ÿå†…
                return True
        
        return False
    
    async def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        logger.info("ğŸ” å¼€å§‹ç›‘æ§å¼€ç›˜æ—¶é—´è‚¡ç¥¨æ¨é€...")
        self.monitoring = True
        self.stats['monitoring_start'] = time.time()
        
        try:
            while self.monitoring:
                current_time = datetime.now().strftime('%H:%M:%S')
                
                if self.is_opening_time():
                    logger.info(f"ğŸš¨ æ£€æµ‹åˆ°å¼€ç›˜æ—¶é—´ {current_time} - å¯åŠ¨é«˜é¢‘ç›‘æ§")
                    await self.high_frequency_monitoring()
                elif self.is_market_open():
                    logger.info(f"ğŸ“Š äº¤æ˜“æ—¶é—´ {current_time} - æ­£å¸¸ç›‘æ§")
                    await self.normal_monitoring()
                else:
                    logger.info(f"ğŸ˜´ éäº¤æ˜“æ—¶é—´ {current_time} - ä½é¢‘ç›‘æ§")
                    await self.low_frequency_monitoring()
                
                # çŸ­æš‚ä¼‘æ¯
                await asyncio.sleep(10)
                
        except KeyboardInterrupt:
            logger.info("â¹ï¸ ç›‘æ§è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            logger.error(f"âŒ ç›‘æ§å¼‚å¸¸: {e}")
        finally:
            self.monitoring = False
            await self.generate_monitoring_report()
    
    async def high_frequency_monitoring(self):
        """é«˜é¢‘ç›‘æ§ï¼ˆå¼€ç›˜æ—¶é—´ï¼‰"""
        logger.info("ğŸš€ å¯åŠ¨é«˜é¢‘ç›‘æ§æ¨¡å¼...")
        
        start_time = time.time()
        files_detected = 0
        stocks_processed = 0
        
        # è¿ç»­ç›‘æ§5åˆ†é’Ÿ
        while time.time() - start_time < 300:  # 5åˆ†é’Ÿ
            # æ£€æŸ¥æ–°æ–‡ä»¶
            new_files = await self.detect_new_files()
            
            if new_files:
                files_detected += len(new_files)
                logger.info(f"ğŸ“„ æ£€æµ‹åˆ° {len(new_files)} ä¸ªæ–°æ–‡ä»¶")
                
                # ç«‹å³å¤„ç†
                processed = await self.process_files_immediately(new_files)
                stocks_processed += processed
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['total_files_detected'] += len(new_files)
                self.stats['total_stocks_processed'] += processed
            
            # é«˜é¢‘æ£€æŸ¥ï¼ˆæ¯ç§’ï¼‰
            await asyncio.sleep(1)
        
        # æ›´æ–°å³°å€¼ç»Ÿè®¡
        files_per_minute = files_detected / 5
        stocks_per_minute = stocks_processed / 5
        
        if files_per_minute > self.stats['peak_files_per_minute']:
            self.stats['peak_files_per_minute'] = files_per_minute
        
        if stocks_per_minute > self.stats['peak_stocks_per_minute']:
            self.stats['peak_stocks_per_minute'] = stocks_per_minute
        
        logger.info(f"ğŸ“Š é«˜é¢‘ç›‘æ§å®Œæˆ: {files_detected} æ–‡ä»¶, {stocks_processed} è‚¡ç¥¨")
    
    async def normal_monitoring(self):
        """æ­£å¸¸ç›‘æ§ï¼ˆäº¤æ˜“æ—¶é—´ï¼‰"""
        # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
        new_files = await self.detect_new_files()
        
        if new_files:
            logger.info(f"ğŸ“„ æ£€æµ‹åˆ° {len(new_files)} ä¸ªæ–°æ–‡ä»¶")
            processed = await self.process_files_immediately(new_files)
            
            self.stats['total_files_detected'] += len(new_files)
            self.stats['total_stocks_processed'] += processed
        
        await asyncio.sleep(30)
    
    async def low_frequency_monitoring(self):
        """ä½é¢‘ç›‘æ§ï¼ˆéäº¤æ˜“æ—¶é—´ï¼‰"""
        # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        new_files = await self.detect_new_files()
        
        if new_files:
            logger.info(f"ğŸ“„ éäº¤æ˜“æ—¶é—´æ£€æµ‹åˆ° {len(new_files)} ä¸ªæ–‡ä»¶")
            # éäº¤æ˜“æ—¶é—´å¯ä»¥å»¶è¿Ÿå¤„ç†
            await asyncio.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿå†å¤„ç†
            
            processed = await self.process_files_immediately(new_files)
            self.stats['total_files_detected'] += len(new_files)
            self.stats['total_stocks_processed'] += processed
        
        await asyncio.sleep(300)  # 5åˆ†é’Ÿ
    
    async def detect_new_files(self) -> List[Path]:
        """æ£€æµ‹æ–°æ–‡ä»¶"""
        try:
            if not self.data_dir.exists():
                return []
            
            # è·å–æœ€è¿‘1åˆ†é’Ÿå†…çš„æ–‡ä»¶
            cutoff_time = time.time() - 60
            new_files = []
            
            for file_path in self.data_dir.iterdir():
                if file_path.is_file() and file_path.suffix in ['.dat', '.pkl']:
                    if file_path.stat().st_mtime > cutoff_time:
                        new_files.append(file_path)
            
            return new_files
            
        except Exception as e:
            logger.error(f"âŒ æ£€æµ‹æ–°æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    async def process_files_immediately(self, files: List[Path]) -> int:
        """ç«‹å³å¤„ç†æ–‡ä»¶"""
        total_processed = 0
        
        try:
            # å¯¼å…¥å¤§é‡è‚¡ç¥¨å¤„ç†å™¨
            from mass_stock_database_processor import MassStockDatabaseProcessor
            
            processor = MassStockDatabaseProcessor()
            
            for file_path in files:
                try:
                    # è¯»å–æ–‡ä»¶æ•°æ®
                    stock_data_list = await processor.read_file_data(file_path)
                    
                    if stock_data_list:
                        # æ‰¹é‡å¤„ç†
                        await processor.process_stock_batch(stock_data_list, file_path.name)
                        total_processed += len(stock_data_list)
                        
                        logger.info(f"âœ… å¤„ç†å®Œæˆ: {file_path.name} ({len(stock_data_list)} è‚¡ç¥¨)")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
                    self.stats['errors'] += 1
            
            return total_processed
            
        except Exception as e:
            logger.error(f"âŒ ç«‹å³å¤„ç†å¤±è´¥: {e}")
            self.stats['errors'] += 1
            return 0
    
    async def check_database_status(self):
        """æ£€æŸ¥æ•°æ®åº“çŠ¶æ€"""
        try:
            headers = {
                'Authorization': f'Bearer {SUPABASE_KEY}',
                'apikey': SUPABASE_KEY
            }
            
            # æ£€æŸ¥æœ€è¿‘çš„æ•°æ®å†™å…¥
            response = requests.get(
                f'{SUPABASE_URL}/rest/v1/system_config?key=like.mass_stock_%&order=key.desc&limit=10',
                headers=headers,
                timeout=10
            )
            
            if response.status_code == 200:
                recent_data = response.json()
                logger.info(f"ğŸ“Š æ•°æ®åº“çŠ¶æ€æ­£å¸¸ï¼Œæœ€è¿‘æœ‰ {len(recent_data)} æ¡è®°å½•")
                return True
            else:
                logger.warning(f"âš ï¸ æ•°æ®åº“æŸ¥è¯¢å¼‚å¸¸: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def generate_monitoring_report(self):
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        duration = time.time() - self.stats['monitoring_start']
        
        report = f"""
ğŸ” å¼€ç›˜æ—¶é—´è‚¡ç¥¨æ¨é€ç›‘æ§æŠ¥å‘Š
{'=' * 50}

â±ï¸ ç›‘æ§æ—¶é•¿: {duration / 3600:.2f} å°æ—¶

ğŸ“Š ç›‘æ§ç»Ÿè®¡:
  - æ£€æµ‹åˆ°æ–‡ä»¶æ€»æ•°: {self.stats['total_files_detected']}
  - å¤„ç†è‚¡ç¥¨æ€»æ•°: {self.stats['total_stocks_processed']}
  - æ•°æ®åº“å†™å…¥æ¬¡æ•°: {self.stats['database_writes']}
  - é”™è¯¯æ¬¡æ•°: {self.stats['errors']}

ğŸš€ å³°å€¼æ€§èƒ½:
  - å³°å€¼æ–‡ä»¶/åˆ†é’Ÿ: {self.stats['peak_files_per_minute']:.2f}
  - å³°å€¼è‚¡ç¥¨/åˆ†é’Ÿ: {self.stats['peak_stocks_per_minute']:.2f}

ğŸ“ˆ å¹³å‡æ€§èƒ½:
  - å¹³å‡æ–‡ä»¶/å°æ—¶: {self.stats['total_files_detected'] / (duration / 3600):.2f}
  - å¹³å‡è‚¡ç¥¨/å°æ—¶: {self.stats['total_stocks_processed'] / (duration / 3600):.2f}

ğŸ¯ ç³»ç»ŸçŠ¶æ€:
  - ç›‘æ§æ¨¡å¼: å¼€ç›˜æ—¶é—´é«˜é¢‘ + äº¤æ˜“æ—¶é—´æ­£å¸¸ + éäº¤æ˜“æ—¶é—´ä½é¢‘
  - æ•°æ®å­˜å‚¨: Supabase system_configè¡¨
  - å¤„ç†ç­–ç•¥: å®æ—¶æ£€æµ‹ + ç«‹å³å¤„ç†

{'=' * 50}
"""
        
        logger.info(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f'opening_time_monitoring_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ğŸ“„ ç›‘æ§æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        logger.info("â¹ï¸ ç›‘æ§å·²åœæ­¢")

async def main():
    """ä¸»å‡½æ•°"""
    monitor = OpeningTimeMonitor()
    
    logger.info("ğŸš€ å¯åŠ¨å¼€ç›˜æ—¶é—´è‚¡ç¥¨æ¨é€ç›‘æ§å™¨...")
    
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        db_ok = await monitor.check_database_status()
        if not db_ok:
            logger.warning("âš ï¸ æ•°æ®åº“è¿æ¥å¼‚å¸¸ï¼Œä½†ç»§ç»­ç›‘æ§")
        
        # å¼€å§‹ç›‘æ§
        await monitor.start_monitoring()
        
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ ç›‘æ§è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ ç›‘æ§å¼‚å¸¸: {e}")
    finally:
        monitor.stop_monitoring()

if __name__ == '__main__':
    asyncio.run(main())
